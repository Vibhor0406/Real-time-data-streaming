
# Real time Data Streaming Pipeline

This project serves as a comprehensive guide to building an end-to-end data engineering pipeline. It covers each stage from data ingestion to processing and finally to storage, utilizing a robust tech stack that includes:

- **Apache Airflow**: Orchestrates the entire workflow
- **Python**: Core scripting language for various tasks
- **Apache Kafka**: Handles real-time data streaming
- **Apache Zookeeper**: Coordinates distributed applications
- **Apache Spark**: Processes large-scale data efficiently
- **Cassandra**: Stores data in a scalable and distributed manner

Everything is containerized using Docker for ease of deployment and scalability.

## System Architecture

The system architecture is designed to be modular and scalable, ensuring each component can be developed, tested, and deployed independently.
---
